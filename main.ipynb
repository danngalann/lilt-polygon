{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b9a3d1309efd32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9409a00d57a0ff8d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "datasets_root = 'datasets'\n",
    "dataset_id = 'FUNSD_polygon_augmented'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:44:10.031568300Z",
     "start_time": "2023-09-25T15:44:10.025567300Z"
    }
   },
   "id": "c096c7b419b6b9d4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import LayoutLMv3FeatureExtractor, AutoTokenizer, LayoutLMv3Processor\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D\n",
    "from PIL import Image\n",
    "from functools import partial"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:44:13.281936200Z",
     "start_time": "2023-09-25T15:44:10.782936100Z"
    }
   },
   "id": "c62815d2ce12da1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataset\n",
    "Generate a Dataset object from a generator function that contains the paths of the images and annotations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1909f5b165b18a88"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def generate_samples(folder):\n",
    "    images_path = Path(folder) / 'images'\n",
    "    annotations_path = Path(folder) / 'annotations'\n",
    "    \n",
    "    images = list(images_path.glob('*.png'))\n",
    "    \n",
    "    for image_path in images:\n",
    "        image_id = image_path.stem\n",
    "        annotation_path = annotations_path / f'{image_id}.json'\n",
    "        yield {\n",
    "            'image_path': str(image_path),\n",
    "            'annotation_path': str(annotation_path)\n",
    "        }\n",
    "\n",
    "def get_features_type():\n",
    "    return Features({\n",
    "        \"image_path\": Value(\"string\"),\n",
    "        \"annotation_path\": Value(\"string\")\n",
    "    })\n",
    "\n",
    "def get_dataset_folder(split='train'):\n",
    "    if split == 'train':\n",
    "        path = Path(datasets_root) / dataset_id / 'dataset' / 'training_data'\n",
    "    elif split == 'test':\n",
    "        path = Path(datasets_root) / dataset_id / 'dataset' / 'testing_data'\n",
    "    else:\n",
    "        raise ValueError('split must be either \"train\" or \"test\"')\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "train_folder = get_dataset_folder()\n",
    "train_dataset = Dataset.from_generator(partial(generate_samples, train_folder), features=get_features_type())\n",
    "test_folder = get_dataset_folder('test')\n",
    "test_dataset = Dataset.from_generator(partial(generate_samples, test_folder), features=get_features_type())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:44:13.392935Z",
     "start_time": "2023-09-25T15:44:13.284935200Z"
    }
   },
   "id": "1592bd60aa0a77ee"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_unique_labels(folder):\n",
    "    annotations_path = Path(folder) / 'annotations'\n",
    "    unique_labels = set()\n",
    "\n",
    "    for annotation_file in annotations_path.glob('*.json'):\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            annotations = json.load(f)['form']\n",
    "            for annotation in annotations:\n",
    "                unique_labels.add(annotation['label'])\n",
    "\n",
    "    return sorted(list(unique_labels))\n",
    "\n",
    "labels = get_unique_labels(train_folder)\n",
    "num_labels = len(labels)\n",
    "label_to_id = {label: idx for idx, label in enumerate(labels)}\n",
    "id_to_label = {idx: label for idx, label in enumerate(labels)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:44:23.828963100Z",
     "start_time": "2023-09-25T15:44:13.392935Z"
    }
   },
   "id": "a6a5ba4453385e7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess dataset\n",
    "Load the images and annotations, and encode them using the LayoutLMv3 processor."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9afc0e7b222553c"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/14900 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f8b9f0a7f2a4d11a5eb5952b79cb35c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([512, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 65\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m encoding\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# Process your dataset (replace `dataset` with the actual dataset object)\u001B[39;00m\n\u001B[1;32m---> 65\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprocessor\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage_path\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mannotation_path\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mwith_format(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     70\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m test_dataset\u001B[38;5;241m.\u001B[39mmap(\n\u001B[0;32m     71\u001B[0m     partial(process, processor\u001B[38;5;241m=\u001B[39mprocessor),\n\u001B[0;32m     72\u001B[0m     remove_columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_path\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mannotation_path\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     73\u001B[0m     features\u001B[38;5;241m=\u001B[39mfeatures,\n\u001B[0;32m     74\u001B[0m )\u001B[38;5;241m.\u001B[39mwith_format(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:592\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    590\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 592\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    593\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    550\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    555\u001B[0m }\n\u001B[0;32m    556\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 557\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    558\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    559\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3090\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3091\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[0;32m   3092\u001B[0m         disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[0;32m   3093\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3094\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3095\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3096\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3097\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3098\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3099\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3450\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3448\u001B[0m _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   3449\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[1;32m-> 3450\u001B[0m     example \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3451\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m update_data:\n\u001B[0;32m   3452\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3353\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   3352\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 3353\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[0;32m   3354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[0;32m   3355\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3356\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[0;32m   3357\u001B[0m     }\n",
      "Cell \u001B[1;32mIn[16], line 46\u001B[0m, in \u001B[0;36mprocess\u001B[1;34m(sample, processor)\u001B[0m\n\u001B[0;32m     43\u001B[0m sequence_labels \u001B[38;5;241m=\u001B[39m sequence_labels \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(padding_length)]\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Encoding without \u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m encoding \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m    \u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m    \u001B[49m\u001B[43mword_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msequence_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m    \u001B[49m\u001B[43mboxes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolygons\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# Manually insert polygons (since processor can't handle 8-coordinates)\u001B[39;00m\n\u001B[0;32m     55\u001B[0m encoding[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpolygon\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m polygons\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py:114\u001B[0m, in \u001B[0;36mLayoutLMv3Processor.__call__\u001B[1;34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    110\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot provide word labels if you initialized the image processor with apply_ocr set to True.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    111\u001B[0m     )\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# first, apply the image processor\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;66;03m# second, apply the tokenizer\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_processor\u001B[38;5;241m.\u001B[39mapply_ocr \u001B[38;5;129;01mand\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\transformers\\image_processing_utils.py:546\u001B[0m, in \u001B[0;36mBaseImageProcessor.__call__\u001B[1;34m(self, images, **kwargs)\u001B[0m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchFeature:\n\u001B[0;32m    545\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 546\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\transformers\\models\\layoutlmv3\\image_processing_layoutlmv3.py:317\u001B[0m, in \u001B[0;36mLayoutLMv3ImageProcessor.preprocess\u001B[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, apply_ocr, ocr_lang, tesseract_config, return_tensors, data_format, input_data_format, **kwargs)\u001B[0m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf do_normalize is True, image_mean and image_std must be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    316\u001B[0m \u001B[38;5;66;03m# All transformations expect numpy arrays.\u001B[39;00m\n\u001B[1;32m--> 317\u001B[0m images \u001B[38;5;241m=\u001B[39m [to_numpy_array(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_scaled_image(images[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m do_rescale:\n\u001B[0;32m    320\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    323\u001B[0m     )\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\transformers\\models\\layoutlmv3\\image_processing_layoutlmv3.py:317\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf do_normalize is True, image_mean and image_std must be specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    316\u001B[0m \u001B[38;5;66;03m# All transformations expect numpy arrays.\u001B[39;00m\n\u001B[1;32m--> 317\u001B[0m images \u001B[38;5;241m=\u001B[39m [\u001B[43mto_numpy_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_scaled_image(images[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m do_rescale:\n\u001B[0;32m    320\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    323\u001B[0m     )\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\transformers\\image_utils.py:156\u001B[0m, in \u001B[0;36mto_numpy_array\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(img)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_vision_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage):\n\u001B[1;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m to_numpy(img)\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\PIL\\Image.py:673\u001B[0m, in \u001B[0;36mImage.__array_interface__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m         new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtobytes(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 673\u001B[0m         new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtobytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, (\u001B[38;5;167;01mMemoryError\u001B[39;00m, \u001B[38;5;167;01mRecursionError\u001B[39;00m)):\n",
      "File \u001B[1;32mS:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\PIL\\Image.py:753\u001B[0m, in \u001B[0;36mImage.tobytes\u001B[1;34m(self, encoder_name, *args)\u001B[0m\n\u001B[0;32m    750\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder error \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merrcode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in tobytes\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    751\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n\u001B[1;32m--> 753\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize processor\n",
    "model_id = \"SCUT-DLVCLab/lilt-roberta-en-base\"\n",
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer)\n",
    "\n",
    "# Custom features\n",
    "features = Features(\n",
    "    {\n",
    "        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"attention_mask\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"polygon\": Array2D(dtype=\"int64\", shape=(512, 8)),\n",
    "        \"labels\": Sequence(feature=ClassLabel(names=labels)),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Preprocess function\n",
    "def process(sample, processor=None):\n",
    "    # Assume sample[\"annotation_path\"] is the path to the JSON file containing your annotations\n",
    "    with open(sample[\"annotation_path\"], \"r\") as f:\n",
    "        annotations = json.load(f)['form']\n",
    "\n",
    "    # Extract sequence and bounding polygons from annotations\n",
    "    sequences = [ann[\"text\"] for ann in annotations]\n",
    "    polygons = [ann[\"polygon\"] for ann in annotations]  # Modify this if your \"polygon\" is not actually a bbox\n",
    "    \n",
    "    max_length = 512\n",
    "    padding_length = max_length - len(sequences)\n",
    "\n",
    "    # Custom Padding for polygons\n",
    "    polygons = polygons + [[0, 0, 0, 0, 0, 0, 0, 0] for _ in range(padding_length)]\n",
    "    \n",
    "    # Custom Padding for sequence\n",
    "    sequences = sequences + [\"[PAD]\" for _ in range(padding_length)]\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "    \n",
    "    # Extract labels for each sequence from annotations\n",
    "    sequence_labels = [label_to_id[ann['label']] for ann in annotations]    \n",
    "    sequence_labels = sequence_labels + [-100 for _ in range(padding_length)]\n",
    "\n",
    "    # Encoding without \n",
    "    encoding = processor(\n",
    "        image,\n",
    "        sequences,\n",
    "        word_labels=sequence_labels,\n",
    "        boxes=polygons,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    # Manually insert polygons (since processor can't handle 8-coordinates)\n",
    "    encoding['polygon'] = polygons\n",
    "    \n",
    "    del encoding[\"pixel_values\"]\n",
    "    del encoding['bbox']\n",
    "\n",
    "    return encoding\n",
    "\n",
    "# Process your dataset (replace `dataset` with the actual dataset object)\n",
    "train_dataset = train_dataset.map(\n",
    "    partial(process, processor=processor),\n",
    "    remove_columns=[\"image_path\", \"annotation_path\"],\n",
    "    features=features,\n",
    ").with_format(\"torch\")\n",
    "test_dataset = test_dataset.map(\n",
    "    partial(process, processor=processor),\n",
    "    remove_columns=[\"image_path\", \"annotation_path\"],\n",
    "    features=features,\n",
    ").with_format(\"torch\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T16:06:05.766643Z",
     "start_time": "2023-09-25T16:06:03.384642300Z"
    }
   },
   "id": "5e7fb18673b75994"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/2 shards):   0%|          | 0/14900 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dba90f9fe27041abb450f9acdfbfdc99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fa74662265c463087251cd2a85d3e28"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "dataset.save_to_disk(Path(datasets_root) / dataset_id / 'huggingface_dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T15:32:28.215730200Z",
     "start_time": "2023-09-25T15:32:26.922047Z"
    }
   },
   "id": "a574d7c6e1f5a373"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d424c1bd12716d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load HF dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b939a67b23b9717"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'polygon', 'labels'],\n        num_rows: 14900\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'polygon', 'labels'],\n        num_rows: 5000\n    })\n})"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict.load_from_disk(f'{datasets_root}/{dataset_id}/huggingface_dataset')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T16:15:58.333841100Z",
     "start_time": "2023-09-25T16:15:58.241840300Z"
    }
   },
   "id": "74de563222080a89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "302cfcb7c774a575"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from model.lilt import  LiltForTokenClassification\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    'SCUT-DLVCLab/lilt-roberta-en-base',\n",
    "    num_labels=num_labels,\n",
    "    label2id=label_to_id,\n",
    "    id2label=id_to_label,\n",
    "    problem_type=\"single_label_classification\",\n",
    ")\n",
    "# Overriding specific configurations\n",
    "# config.hidden_size = 768\n",
    "# config.max_2d_position_embeddings = 1024\n",
    "# config.channel_shrink_ratio = 2\n",
    "model = LiltForTokenClassification(config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T16:16:01.208661300Z",
     "start_time": "2023-09-25T16:15:59.931630400Z"
    }
   },
   "id": "67cebd70dc102ea1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2869bc96f0250a3f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# labels of the model\n",
    "class_labels = [config.id2label[i] for i in range(num_labels)]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for predicted_idx, label_idx in zip(prediction, label):\n",
    "            if label_idx == -100:\n",
    "                continue\n",
    "            all_predictions.append(class_labels[predicted_idx])\n",
    "            all_labels.append(class_labels[label_idx])\n",
    "    return metric.compute(predictions=[all_predictions], references=[all_labels])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T16:16:03.028177900Z",
     "start_time": "2023-09-25T16:16:02.113470700Z"
    }
   },
   "id": "eb6cc62d4a480725"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare trainer & train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e3062c2f7bebe8a"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# hugging face parameter\n",
    "repository_id = \"lilt-polygon\"\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    max_steps=2500,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T16:16:06.370082700Z",
     "start_time": "2023-09-25T16:16:06.053083300Z"
    }
   },
   "id": "f73cd82e58d3e022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2500 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-25T16:16:08.664463100Z"
    }
   },
   "id": "b13517dd7da566f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
