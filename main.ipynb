{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9409a00d57a0ff8d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "datasets_root = 'datasets'\n",
    "dataset_id = 'FUNSD_polygon_augmented'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:58:53.523174700Z",
     "start_time": "2023-09-25T08:58:53.509155600Z"
    }
   },
   "id": "c096c7b419b6b9d4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import LayoutLMv3FeatureExtractor, AutoTokenizer, LayoutLMv3Processor\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D\n",
    "from PIL import Image\n",
    "from functools import partial"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:58:55.587970100Z",
     "start_time": "2023-09-25T08:58:53.516174300Z"
    }
   },
   "id": "c62815d2ce12da1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataset\n",
    "Generate a Dataset object from a generator function that contains the paths of the images and annotations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1909f5b165b18a88"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b2fafebfd444c03994852971241ff36"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(folder):\n",
    "    images_path = Path(folder) / 'images'\n",
    "    annotations_path = Path(folder) / 'annotations'\n",
    "    \n",
    "    images = list(images_path.glob('*.png'))\n",
    "    \n",
    "    for image_path in images:\n",
    "        image_id = image_path.stem\n",
    "        annotation_path = annotations_path / f'{image_id}.json'\n",
    "        yield {\n",
    "            'image_path': str(image_path),\n",
    "            'annotation_path': str(annotation_path)\n",
    "        }\n",
    "\n",
    "def get_features_type():\n",
    "    return Features({\n",
    "        \"image_path\": Value(\"string\"),\n",
    "        \"annotation_path\": Value(\"string\")\n",
    "    })\n",
    "\n",
    "def get_dataset_folder(split='train'):\n",
    "    if split == 'train':\n",
    "        path = Path(datasets_root) / dataset_id / 'dataset' / 'training_data'\n",
    "    elif split == 'test':\n",
    "        path = Path(datasets_root) / dataset_id / 'dataset' / 'testing_data'\n",
    "    else:\n",
    "        raise ValueError('split must be either \"train\" or \"test\"')\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "train_folder = get_dataset_folder()\n",
    "train_dataset = Dataset.from_generator(partial(generate_samples, train_folder), features=get_features_type())\n",
    "test_folder = get_dataset_folder('test')\n",
    "test_dataset = Dataset.from_generator(partial(generate_samples, test_folder), features=get_features_type())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:58:56.391380700Z",
     "start_time": "2023-09-25T08:58:55.589971100Z"
    }
   },
   "id": "1592bd60aa0a77ee"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_unique_labels(folder):\n",
    "    annotations_path = Path(folder) / 'annotations'\n",
    "    unique_labels = set()\n",
    "\n",
    "    for annotation_file in annotations_path.glob('*.json'):\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            annotations = json.load(f)['form']\n",
    "            for annotation in annotations:\n",
    "                unique_labels.add(annotation['label'])\n",
    "\n",
    "    return sorted(list(unique_labels))\n",
    "\n",
    "labels = get_unique_labels(train_folder)\n",
    "label_to_id = {label: idx for idx, label in enumerate(labels)}\n",
    "id_to_label = {idx: label for idx, label in enumerate(labels)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:59:06.205379200Z",
     "start_time": "2023-09-25T08:58:56.392379400Z"
    }
   },
   "id": "a6a5ba4453385e7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess dataset\n",
    "Load the images and annotations, and encode them using the LayoutLMv3 processor."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9afc0e7b222553c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\Projects\\polygon-document-comprehension\\venv\\lib\\site-packages\\transformers\\models\\layoutlmv3\\feature_extraction_layoutlmv3.py:30: FutureWarning: The class LayoutLMv3FeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use LayoutLMv3ImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0273a7f524664caaa2b3d8ea1fdcfee8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize processor\n",
    "model_id = \"SCUT-DLVCLab/lilt-roberta-en-base\"\n",
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer)\n",
    "\n",
    "# Custom features\n",
    "features = Features(\n",
    "    {\n",
    "        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"attention_mask\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 8)),\n",
    "        \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Preprocess function\n",
    "def process(sample, processor=None):\n",
    "    # Assume sample[\"annotation_path\"] is the path to the JSON file containing your annotations\n",
    "    with open(sample[\"annotation_path\"], \"r\") as f:\n",
    "        annotations = json.load(f)['form']\n",
    "\n",
    "    # Extract tokens and bounding boxes from annotations\n",
    "    tokens = [ann[\"text\"] for ann in annotations]\n",
    "    bboxes = [ann[\"polygon\"] for ann in annotations]  # Modify this if your \"polygon\" is not actually a bbox\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "    \n",
    "    # Convert string labels to integers using the mapping\n",
    "    word_labels = [label_to_id[ann[\"label\"]] for ann in annotations]\n",
    "\n",
    "    # Encoding\n",
    "    encoding = processor(\n",
    "        image,\n",
    "        tokens,\n",
    "        boxes=bboxes,\n",
    "        word_labels=word_labels,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    del encoding[\"pixel_values\"]\n",
    "    return encoding\n",
    "\n",
    "# Process your dataset (replace `dataset` with the actual dataset object)\n",
    "train_dataset = train_dataset.map(\n",
    "    partial(process, processor=processor),\n",
    "    remove_columns=[\"image_path\", \"annotation_path\"],\n",
    "    features=features,\n",
    ").with_format(\"torch\")\n",
    "test_dataset = test_dataset.map(\n",
    "    partial(process, processor=processor),\n",
    "    remove_columns=[\"image_path\", \"annotation_path\"],\n",
    "    features=features,\n",
    ").with_format(\"torch\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T09:01:22.044771600Z",
     "start_time": "2023-09-25T08:59:06.209379300Z"
    }
   },
   "id": "5e7fb18673b75994"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/2 shards):   0%|          | 0/14900 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e0e301527a74efe9e9f0d97649b43f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1192a763faf4f96822c41c3f7056a9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "dataset.save_to_disk(Path(datasets_root) / dataset_id / 'huggingface_dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T09:01:22.865639100Z",
     "start_time": "2023-09-25T09:01:22.044771600Z"
    }
   },
   "id": "a574d7c6e1f5a373"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
