{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b9a3d1309efd32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9409a00d57a0ff8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datasets_root = 'datasets'\n",
    "dataset_id = 'FUNSD_polygon_augmented'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c096c7b419b6b9d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import LayoutLMv3FeatureExtractor, AutoTokenizer, LayoutLMv3Processor\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D\n",
    "from PIL import Image\n",
    "from functools import partial"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c62815d2ce12da1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load dataset\n",
    "Generate a Dataset object from a generator function that contains the paths of the images and annotations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1909f5b165b18a88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_samples(folder):\n",
    "    images_path = Path(folder) / 'images'\n",
    "    annotations_path = Path(folder) / 'annotations'\n",
    "    \n",
    "    images = list(images_path.glob('*.png'))\n",
    "    \n",
    "    for image_path in images:\n",
    "        image_id = image_path.stem\n",
    "        annotation_path = annotations_path / f'{image_id}.json'\n",
    "        yield {\n",
    "            'image_path': str(image_path),\n",
    "            'annotation_path': str(annotation_path)\n",
    "        }\n",
    "\n",
    "def get_features_type():\n",
    "    return Features({\n",
    "        \"image_path\": Value(\"string\"),\n",
    "        \"annotation_path\": Value(\"string\")\n",
    "    })\n",
    "\n",
    "def get_dataset_folder(split='train'):\n",
    "    if split == 'train':\n",
    "        path = Path(datasets_root) / dataset_id / 'dataset' / 'training_data'\n",
    "    elif split == 'test':\n",
    "        path = Path(datasets_root) / dataset_id / 'dataset' / 'testing_data'\n",
    "    else:\n",
    "        raise ValueError('split must be either \"train\" or \"test\"')\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "train_folder = get_dataset_folder()\n",
    "train_dataset = Dataset.from_generator(partial(generate_samples, train_folder), features=get_features_type())\n",
    "test_folder = get_dataset_folder('test')\n",
    "test_dataset = Dataset.from_generator(partial(generate_samples, test_folder), features=get_features_type())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1592bd60aa0a77ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_unique_labels(folder):\n",
    "    annotations_path = Path(folder) / 'annotations'\n",
    "    unique_labels = set()\n",
    "\n",
    "    for annotation_file in annotations_path.glob('*.json'):\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            annotations = json.load(f)['form']\n",
    "            for annotation in annotations:\n",
    "                unique_labels.add(annotation['label'])\n",
    "\n",
    "    return sorted(list(unique_labels))\n",
    "\n",
    "labels = get_unique_labels(train_folder)\n",
    "label_to_id = {label: idx for idx, label in enumerate(labels)}\n",
    "id_to_label = {idx: label for idx, label in enumerate(labels)}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6a5ba4453385e7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess dataset\n",
    "Load the images and annotations, and encode them using the LayoutLMv3 processor."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9afc0e7b222553c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "model_id = \"SCUT-DLVCLab/lilt-roberta-en-base\"\n",
    "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "processor = LayoutLMv3Processor(feature_extractor, tokenizer)\n",
    "\n",
    "# Custom features\n",
    "features = Features(\n",
    "    {\n",
    "        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"attention_mask\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 8)),\n",
    "        \"labels\": Sequence(ClassLabel(names=labels)),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Preprocess function\n",
    "def process(sample, processor=None):\n",
    "    # Assume sample[\"annotation_path\"] is the path to the JSON file containing your annotations\n",
    "    with open(sample[\"annotation_path\"], \"r\") as f:\n",
    "        annotations = json.load(f)['form']\n",
    "\n",
    "    # Extract tokens and bounding boxes from annotations\n",
    "    tokens = [ann[\"text\"] for ann in annotations]\n",
    "    bboxes = [ann[\"polygon\"] for ann in annotations]  # Modify this if your \"polygon\" is not actually a bbox\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "    \n",
    "    # Convert string labels to integers using the mapping\n",
    "    word_labels = [label_to_id[ann[\"label\"]] for ann in annotations]\n",
    "\n",
    "    # Encoding\n",
    "    encoding = processor(\n",
    "        image,\n",
    "        tokens,\n",
    "        boxes=bboxes,\n",
    "        word_labels=word_labels,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    del encoding[\"pixel_values\"]\n",
    "    return encoding\n",
    "\n",
    "# Process your dataset (replace `dataset` with the actual dataset object)\n",
    "train_dataset = train_dataset.map(\n",
    "    partial(process, processor=processor),\n",
    "    remove_columns=[\"image_path\", \"annotation_path\"],\n",
    "    features=features,\n",
    ").with_format(\"torch\")\n",
    "test_dataset = test_dataset.map(\n",
    "    partial(process, processor=processor),\n",
    "    remove_columns=[\"image_path\", \"annotation_path\"],\n",
    "    features=features,\n",
    ").with_format(\"torch\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e7fb18673b75994"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "dataset.save_to_disk(Path(datasets_root) / dataset_id / 'huggingface_dataset')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a574d7c6e1f5a373"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d424c1bd12716d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load HF dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b939a67b23b9717"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk(f'{datasets_root}/{dataset_id}/huggingface_dataset')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74de563222080a89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "302cfcb7c774a575"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from model.lilt import LiltForTokenClassification\n",
    "from transformers import AutoConfig\n",
    "\n",
    "num_labels = len(labels)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    'SCUT-DLVCLab/lilt-roberta-en-base',\n",
    "    num_labels=num_labels,\n",
    "    label2id=label_to_id,\n",
    "    id2label=id_to_label,\n",
    ")\n",
    "model = LiltForTokenClassification(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f'Model size: {model_size:,}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67cebd70dc102ea1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2869bc96f0250a3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# labels of the model\n",
    "ner_labels = list(model.config.id2label.values())\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for predicted_idx, label_idx in zip(prediction, label):\n",
    "            if label_idx == -100:\n",
    "                continue\n",
    "            all_predictions.append(ner_labels[predicted_idx])\n",
    "            all_labels.append(ner_labels[label_idx])\n",
    "    return metric.compute(predictions=[all_predictions], references=[all_labels])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb6cc62d4a480725"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare trainer & train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e3062c2f7bebe8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# hugging face parameter\n",
    "repository_id = \"lilt-polygon\"\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-5,\n",
    "    max_steps=2500,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f73cd82e58d3e022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b13517dd7da566f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
